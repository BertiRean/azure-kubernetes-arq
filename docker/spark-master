#!/bin/bash
export SPARK_LOCAL_IP="$(hostname --ip-address)"

# Start bash to prevent Docker image from exit
# exec bash

unset SPARK_MASTER_PORT

# SSH Server Initialization
echo "Initializing SSH"
sudo service ssh start
eval `ssh-agent -s`
exec ssh-add &
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# Below 3 lines will hide the prompt about fingerprinting during ssh connection 
# to HDFS
ssh -oStrictHostKeyChecking=no spark-master uptime
ssh -oStrictHostKeyChecking=no localhost uptime
ssh -oStrictHostKeyChecking=no 0.0.0.0 uptime

# Starting the HDFS
echo "Starting HDFS"
start-dfs.sh > ./start-dfs.log

# Now, start YARN resource manager and redirect output to the logs
echo "Starting YARN resource manager"
yarn resourcemanager > ~/resourcemanager.log 2>&1 &

sudo /bin/bash -c 'echo -e "$(hostname -i) spark-master" >> /etc/hosts'

# Testing Hadoop
echo "Testing Hadoop history server..."
hdfs dfs -mkdir -p /shared
hdfs dfs -mkdir -p /user/sparker/
hdfs dfs -copyFromLocal /home/sparker/TGN_Entrega_GBA_Agosto.csv /user/sparker/
hdfs dfs -mkdir -p /shared/spark-logs 

# Next, start Spark master
echo "Starting Spark master..."
${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.master.Master --ip spark-master --port 7077 --webui-port 8080
